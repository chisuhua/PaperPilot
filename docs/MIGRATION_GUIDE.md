# æ¶æ„è¿ç§»æŒ‡å—

## ğŸ¯ ç›®æ ‡

ä»å½“å‰å®ç°ï¼ˆv1ï¼‰è¿ç§»åˆ° PLAN.md å»ºè®®æ¶æ„ï¼ˆv2ï¼‰ï¼Œå®ç°æ›´å¼ºå¤§ã€å¯æ‰©å±•çš„è®ºæ–‡ç®¡ç†ç³»ç»Ÿã€‚

---

## ğŸ“Š å…³é”®å·®å¼‚é€ŸæŸ¥è¡¨

### æ ¸å¿ƒå˜æ›´

| ç»„ä»¶ | v1 (å½“å‰) | v2 (PLAN.md) | è¿ç§»éš¾åº¦ |
|------|-----------|--------------|---------|
| PDF è§£æ | PyPDF2 | pymupdf | ğŸŸ¢ ç®€å• |
| åµŒå…¥æ¨¡å‹ | MiniLM-L12-v2 | bge-m3 | ğŸŸ¢ ç®€å• |
| å‘é‡å­˜å‚¨ | numpy æ•°ç»„ | ChromaDB | ğŸŸ¡ ä¸­ç­‰ |
| åˆ†å—ç­–ç•¥ | æ®µè½åˆ†å‰² | å›ºå®šé•¿åº¦+é‡å  | ğŸŸ¢ ç®€å• |
| æŒä¹…åŒ– | âŒ æ—  | âœ… è‡ªåŠ¨ | ğŸŸ¢ ç®€å• |

---

## ğŸ”„ é€æ­¥è¿ç§»æ–¹æ¡ˆ

### æ–¹æ¡ˆ Aï¼šæ¸è¿›å¼è¿ç§»ï¼ˆæ¨èï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šç”Ÿäº§ç¯å¢ƒæœ‰ç°æœ‰ç”¨æˆ·

```
Week 1: æ–°å»º v2 åˆ†æ”¯ï¼Œå®ç°æ ¸å¿ƒåŠŸèƒ½
Week 2: å¹¶è¡Œè¿è¡Œ v1 å’Œ v2ï¼ŒA/B æµ‹è¯•
Week 3: æ•°æ®è¿ç§»ï¼Œå…¨é¢åˆ‡æ¢åˆ° v2
```

**ä¼˜ç‚¹**ï¼š
- âœ… é£é™©å¯æ§
- âœ… ç”¨æˆ·æ— æ„ŸçŸ¥
- âœ… å¯éšæ—¶å›æ»š

**ç¼ºç‚¹**ï¼š
- âš ï¸ éœ€è¦ç»´æŠ¤ä¸¤å¥—ä»£ç 
- âš ï¸ è¿ç§»å‘¨æœŸè¾ƒé•¿

---

### æ–¹æ¡ˆ Bï¼šç›´æ¥é‡å†™ï¼ˆå¿«é€Ÿï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šæ— ç”Ÿäº§æ•°æ®ï¼Œæˆ–ç”¨æˆ·å¯å®¹å¿åœæœº

```
Day 1-2: æŒ‰ PLAN.md å®ç°æ–°æ¶æ„
Day 3: æµ‹è¯•éªŒè¯
Day 4: ä¸€æ¬¡æ€§åˆ‡æ¢
```

**ä¼˜ç‚¹**ï¼š
- âœ… å¿«é€Ÿå®Œæˆ
- âœ… ä»£ç ç®€æ´

**ç¼ºç‚¹**ï¼š
- âš ï¸ æœ‰åœæœºæ—¶é—´
- âš ï¸ éœ€è¦æ•°æ®è¿ç§»è„šæœ¬

---

## ğŸ“ è¯¦ç»†è¿ç§»æ­¥éª¤

### Step 1: ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…æ–°ä¾èµ–
pip install pymupdf chromadb

# å¯é€‰ï¼šåˆ›å»ºè™šæ‹Ÿç¯å¢ƒéš”ç¦»æµ‹è¯•
python -m venv venv_v2
source venv_v2/bin/activate
pip install -r requirements_v2.txt
```

### Step 2: åˆ›å»ºæ–°æ¨¡å—ç»“æ„

```
PaperPilot/
â”œâ”€â”€ v1/                    # ä¿ç•™å½“å‰å®ç°
â”‚   â”œâ”€â”€ pdf_processor.py
â”‚   â”œâ”€â”€ search_engine.py
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ v2/                    # æ–°æ¶æ„
â”‚   â”œâ”€â”€ pdf_extractor.py   # ä½¿ç”¨ pymupdf
â”‚   â”œâ”€â”€ text_chunker.py    # å›ºå®šé•¿åº¦åˆ†å—
â”‚   â”œâ”€â”€ vector_store.py    # ChromaDB å°è£…
â”‚   â””â”€â”€ app.py             # æ›´æ–°çš„ UI
â””â”€â”€ migration/             # è¿ç§»å·¥å…·
    â””â”€â”€ import_from_v1.py
```

### Step 3: å®ç°æ ¸å¿ƒæ¨¡å—

#### 3.1 PDF æå–å™¨ï¼ˆv2/pdf_extractor.pyï¼‰

```python
import fitz  # pymupdf

class PDFExtractor:
    def extract_text(self, pdf_path: str) -> str:
        """ä½¿ç”¨ pymupdf æå–æ–‡æœ¬ï¼Œæ¯” PyPDF2 æ›´é²æ£’"""
        doc = fitz.open(pdf_path)
        text = ""
        for page in doc:
            text += page.get_text()
        doc.close()
        return text
    
    def extract_with_metadata(self, pdf_path: str) -> dict:
        """åŒæ—¶æå–æ–‡æœ¬å’Œå…ƒæ•°æ®"""
        doc = fitz.open(pdf_path)
        return {
            'text': ''.join(page.get_text() for page in doc),
            'pages': len(doc),
            'metadata': doc.metadata
        }
```

#### 3.2 æ–‡æœ¬åˆ†å—å™¨ï¼ˆv2/text_chunker.pyï¼‰

```python
class TextChunker:
    def __init__(self, chunk_size=512, overlap=50):
        self.chunk_size = chunk_size
        self.overlap = overlap
    
    def chunk(self, text: str) -> list[str]:
        """å›ºå®šé•¿åº¦åˆ†å—ï¼Œå¸¦é‡å """
        chunks = []
        start = 0
        while start < len(text):
            end = start + self.chunk_size
            chunks.append(text[start:end])
            start += self.chunk_size - self.overlap
        return chunks
```

#### 3.3 å‘é‡å­˜å‚¨ï¼ˆv2/vector_store.pyï¼‰

```python
import chromadb
from sentence_transformers import SentenceTransformer

class VectorStore:
    def __init__(self, collection_name="papers"):
        self.client = chromadb.PersistentClient(path="./chroma_db")
        self.model = SentenceTransformer('BAAI/bge-m3')
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )
    
    def add_documents(self, texts: list[str], metadatas: list[dict]):
        """æ‰¹é‡æ·»åŠ æ–‡æ¡£"""
        embeddings = self.model.encode(texts).tolist()
        ids = [f"doc_{i}" for i in range(len(texts))]
        self.collection.add(
            embeddings=embeddings,
            documents=texts,
            metadatas=metadatas,
            ids=ids
        )
    
    def search(self, query: str, top_k=5):
        """è¯­ä¹‰æœç´¢"""
        query_embedding = self.model.encode([query])[0].tolist()
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k
        )
        return results
```

### Step 4: æ•°æ®è¿ç§»è„šæœ¬

```python
# migration/import_from_v1.py
from v1.pdf_processor import PDFProcessor as ProcessorV1
from v2.pdf_extractor import PDFExtractor
from v2.text_chunker import TextChunker
from v2.vector_store import VectorStore

def migrate_documents():
    """ä» v1 æ•°æ®å¯¼å…¥åˆ° v2"""
    # 1. è¯»å– v1 å·²å¤„ç†çš„æ–‡æ¡£
    processor_v1 = ProcessorV1()
    # ï¼ˆå‡è®¾æœ‰ä¿å­˜çš„æ–‡ä»¶åˆ—è¡¨ï¼‰
    
    # 2. ä½¿ç”¨ v2 ç»„ä»¶é‡æ–°å¤„ç†
    extractor = PDFExtractor()
    chunker = TextChunker()
    store = VectorStore()
    
    for pdf_path in get_v1_documents():
        text = extractor.extract_text(pdf_path)
        chunks = chunker.chunk(text)
        metadatas = [{"source": pdf_path}] * len(chunks)
        store.add_documents(chunks, metadatas)
    
    print("è¿ç§»å®Œæˆï¼")
```

### Step 5: æ›´æ–° Streamlit UI

ä¸»è¦å˜æ›´ï¼š
1. ä½¿ç”¨ `VectorStore` æ›¿ä»£ `SemanticSearchEngine`
2. æ•°æ®è‡ªåŠ¨æŒä¹…åŒ–ï¼ˆæ— éœ€é‡æ–°ç´¢å¼•ï¼‰
3. æ”¯æŒå¢é‡æ·»åŠ æ–‡æ¡£

```python
# v2/app.py å…³é”®å·®å¼‚
if st.button("å¤„ç† PDF"):
    # v1 æ–¹å¼ï¼šå…¨éƒ¨é‡æ–°å¤„ç†
    # processor.process_multiple_pdfs(files)
    
    # v2 æ–¹å¼ï¼šå¢é‡æ·»åŠ 
    for file in new_files:
        text = extractor.extract_text(file)
        chunks = chunker.chunk(text)
        store.add_documents(chunks, [{"source": file.name}])
    st.success("å·²æ·»åŠ åˆ°ç´¢å¼•ï¼")  # è‡ªåŠ¨æŒä¹…åŒ–
```

---

## ğŸ§ª æµ‹è¯•éªŒè¯

### åŠŸèƒ½å¯¹æ¯”æµ‹è¯•

```python
# tests/compare_v1_v2.py
def test_search_quality():
    """å¯¹æ¯” v1 å’Œ v2 çš„æœç´¢ç»“æœ"""
    queries = [
        "æ·±åº¦å­¦ä¹ çš„åº”ç”¨",
        "transformer æ¶æ„",
        "attention mechanism"
    ]
    
    for query in queries:
        results_v1 = search_v1(query)
        results_v2 = search_v2(query)
        
        # å¯¹æ¯”ç›¸ä¼¼åº¦åˆ†æ•°
        # å¯¹æ¯”è¿”å›æ–‡æ¡£çš„ç›¸å…³æ€§
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
import time

def benchmark():
    docs = load_test_documents(count=1000)
    
    # æµ‹è¯•ç´¢å¼•é€Ÿåº¦
    start = time.time()
    index_v1(docs)
    t1 = time.time() - start
    
    start = time.time()
    index_v2(docs)
    t2 = time.time() - start
    
    print(f"v1 ç´¢å¼•æ—¶é—´: {t1:.2f}s")
    print(f"v2 ç´¢å¼•æ—¶é—´: {t2:.2f}s")
```

---

## ğŸ“ æœ€ä½³å®è·µ

### 1. é…ç½®ç®¡ç†

åˆ›å»º `config.yaml` ç»Ÿä¸€ç®¡ç†å‚æ•°ï¼š

```yaml
model:
  name: "BAAI/bge-m3"
  cache_dir: "./models"

chunking:
  chunk_size: 512
  overlap: 50

chromadb:
  persist_directory: "./chroma_db"
  collection_name: "papers"
```

### 2. æ—¥å¿—ç³»ç»Ÿ

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('paperpilot.log'),
        logging.StreamHandler()
    ]
)
```

### 3. é”™è¯¯å¤„ç†

```python
class PDFProcessingError(Exception):
    pass

def safe_extract(pdf_path):
    try:
        return extractor.extract_text(pdf_path)
    except Exception as e:
        logger.error(f"Failed to process {pdf_path}: {e}")
        raise PDFProcessingError(f"Cannot process {pdf_path}")
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: æ¨¡å‹å¤ªå¤§ï¼Œä¸‹è½½å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A**: ä½¿ç”¨å›½å†…é•œåƒæˆ–æ‰‹åŠ¨ä¸‹è½½

```python
# ä½¿ç”¨ HF_ENDPOINT ç¯å¢ƒå˜é‡
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

model = SentenceTransformer('BAAI/bge-m3')
```

### Q2: ChromaDB æ•°æ®æŸåå¦‚ä½•æ¢å¤ï¼Ÿ

**A**: å®šæœŸå¤‡ä»½ + å¯¼å‡ºæœºåˆ¶

```python
# å¯¼å‡ºä¸º JSON
collection.get()  # è·å–æ‰€æœ‰æ•°æ®
# å®šæœŸå¤‡ä»½ ./chroma_db ç›®å½•
```

### Q3: v1 å’Œ v2 èƒ½å¦å…±å­˜ï¼Ÿ

**A**: å¯ä»¥ï¼Œä½¿ç”¨ä¸åŒç«¯å£è¿è¡Œ

```bash
# v1 è¿è¡Œåœ¨ 8501
streamlit run v1/app.py --server.port 8501

# v2 è¿è¡Œåœ¨ 8502
streamlit run v2/app.py --server.port 8502
```

---

## ğŸ“ æ”¯æŒèµ„æº

- æŠ€æœ¯è®¨è®ºï¼š[GitHub Issues](https://github.com/chisuhua/PaperPilot/issues)
- æ¶æ„å¯¹æ¯”æ–‡æ¡£ï¼š`ARCHITECTURE_COMPARISON.md`
- PLAN.mdï¼šå®Œæ•´å¼€å‘è®¡åˆ’

---

*æœ€åæ›´æ–°*ï¼š2026-02-09
